{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Load CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "data_train = pd.read_csv('data_train.csv', encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "data_test = pd.read_csv('data_test.csv',  encoding='latin-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "data_test_w_sent = pd.read_csv('data_test_with_sentiment.csv',  encoding='latin-1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Sampling \n",
    "##### Because the data is not balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "train_sample = data_train.loc[data_train['reviews.sentiment'] == 'negative'].sample(n=1942, random_state=1234)\n",
    "train_sample_positive = data_train.loc[data_train['reviews.sentiment'] == 'positive'].sample(n=1942, random_state=1234)\n",
    "train_sample = train_sample.append(train_sample_positive)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Word Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 1. Remove stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "filtered_sentences = []\n",
    "for i,words in enumerate(train_sample['reviews.text']):\n",
    "    filtered = [word for word in words.split() if word not in stop_words]\n",
    "    filtered_sentences.append(' '.join(filtered))\n",
    "\n",
    "# add new column to store the sentences which stopwords has been removed\n",
    "train_sample['filtered.reviews'] = pd.Series(filtered_sentences, index=train_sample.index)\n",
    "\n",
    "#save filtered sentence data to csv\n",
    "train_sample.to_csv('filtered_train.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "filtered_sentences_test = []\n",
    "for i,words in enumerate(data_test['reviews.text']):\n",
    "    filtered = [word for word in words.split() if word not in stop_words]\n",
    "    filtered_sentences_test.append(' '.join(filtered))\n",
    "\n",
    "# add new column to store the sentences which stopwords has been removed\n",
    "data_test['filtered.reviews'] = pd.Series(filtered_sentences_test, index=data_test.index)\n",
    "\n",
    "#save filtered sentence data to csv\n",
    "data_test.to_csv('filtered_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "filtered_sentences_test_sent = []\n",
    "for i,words in enumerate(data_test_w_sent['reviews.text']):\n",
    "    filtered = [word for word in words.split() if word not in stop_words]\n",
    "    filtered_sentences_test_sent.append(' '.join(filtered))\n",
    "\n",
    "# add new column to store the sentences which stopwords has been removed\n",
    "data_test_w_sent['filtered.reviews'] = pd.Series(filtered_sentences_test_sent, index=data_test_w_sent.index)\n",
    "\n",
    "#save filtered sentence data to csv\n",
    "data_test_w_sent.to_csv('filtered_test_sent.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 2. Remove punctuation + lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "\n",
    "lem = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "filtered_train = train_sample['reviews.text']\n",
    "clean = []\n",
    "for i in filtered_train:\n",
    "    clean_str = re.sub(r'[^\\w\\s]',' ', i) #punctual removal\n",
    "    clean_str = re.sub('\\s+', ' ', clean_str) #remove extra space\n",
    "    words = word_tokenize(clean_str)\n",
    "    for w in range(len(words)):\n",
    "        words[w] = lem.lemmatize(words[w])\n",
    "    sentence = \" \".join(words)\n",
    "\n",
    "    clean.append(sentence)\n",
    "train_sample['lemma'] = clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "filtered_test = data_test['reviews.text']\n",
    "clean_test = []\n",
    "for i in filtered_test:\n",
    "    clean_str = re.sub(r'[^\\w\\s]',' ', i) #punctual removal\n",
    "    clean_str = re.sub('\\s+', ' ', clean_str) #remove extra space\n",
    "    words = word_tokenize(clean_str)\n",
    "    for w in range(len(words)):\n",
    "        words[w] = lem.lemmatize(words[w])\n",
    "    sentence = \" \".join(words)\n",
    "\n",
    "    clean_test.append(sentence)\n",
    "data_test['lemma'] = clean_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "filtered_test_sent = data_test_w_sent['reviews.text']\n",
    "clean_test = []\n",
    "for i in filtered_test_sent:\n",
    "    clean_str = re.sub(r'[^\\w\\s]',' ', i) #punctual removal\n",
    "    clean_str = re.sub('\\s+', ' ', clean_str) #remove extra space\n",
    "    words = word_tokenize(clean_str)\n",
    "    for w in range(len(words)):\n",
    "        words[w] = lem.lemmatize(words[w])\n",
    "    sentence = \" \".join(words)\n",
    "\n",
    "    clean_test.append(sentence)\n",
    "data_test_w_sent['lemma'] = clean_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 3. Stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# snowball stemmer\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "stemmed_train = []\n",
    "for words in train_sample['reviews.text']:\n",
    "    #store each words that have been stemmed in an array of stems\n",
    "    stems = []\n",
    "    for word in words.split():\n",
    "        stem = stemmer.stem(word)\n",
    "        stems.append(stem)\n",
    "    #store each sentences from stems in an array of stemmed_train\n",
    "    stemmed_train.append(' '.join(stems))\n",
    "\n",
    "train_sample['stemmed.reviews'] = pd.Series(stemmed_train, index=train_sample.index)\n",
    "#train_sample.to_csv('stem_train.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "stemmed_test = []\n",
    "for words in data_test['reviews.text']:\n",
    "    #store each words that have been stemmed in an array of stems\n",
    "    stems = []\n",
    "    for word in words.split():\n",
    "        stem = stemmer.stem(word)\n",
    "        stems.append(stem)\n",
    "    #store each sentences from stems in an array of stemmed_train\n",
    "    stemmed_test.append(' '.join(stems))\n",
    "data_test['stemmed_reviews'] = pd.Series(stemmed_test, index=data_test.index)\n",
    "#data_test.to_csv('stem_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "stemmed_test_sent = []\n",
    "for words in data_test_w_sent['reviews.text']:\n",
    "    #store each words that have been stemmed in an array of stems\n",
    "    stems = []\n",
    "    for word in words.split():\n",
    "        stem = stemmer.stem(word)\n",
    "        stems.append(stem)\n",
    "    #store each sentences from stems in an array of stemmed_train\n",
    "    stemmed_test_sent.append(' '.join(stems))\n",
    "data_test_w_sent['stemmed.reviews'] = pd.Series(stemmed_test_sent, index=data_test_w_sent.index)\n",
    "#data_test_w_sent.to_csv('stem_test_w_sent.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 1. TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1,1))\n",
    "X = vectorizer.fit_transform(train_sample['stemmed.reviews'])\n",
    "X_test = vectorizer.transform(data_test['stemmed_reviews'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
   ],
   "source": [
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### 2. Count Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "cv = CountVectorizer(analyzer='word', ngram_range=(1,1))\n",
    "cv.fit(train_sample['filtered.reviews'])\n",
    "X_cv = cv.transform(train_sample['filtered.reviews'])\n",
    "X_test_cv = cv.transform(data_test_w_sent['filtered.reviews'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Training\n",
    "##### Training data is splitted into train & test data for validation before we test the model with the real testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "train_y = train_sample['stemmed.reviews']\n",
    "test_y = data_test['stemmed_reviews']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "validation_size = 0.30\n",
    "seed = 1234 #generate same sample\n",
    "X_train, X_validation, Y_train, Y_validation = train_test_split(X, train_y, test_size=validation_size, random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "from sklearn import naive_bayes\n",
    "import sklearn.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy training :0.8727005150846211\n",
      "Accuracy testing :0.0008576329331046312\n",
      "Confusion Matriks Data Training :\n",
      "[[1 0 0 ... 0 0 0]\n",
      " [0 1 0 ... 0 0 0]\n",
      " [0 0 1 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 1 0 0]\n",
      " [0 0 0 ... 0 1 0]\n",
      " [0 0 0 ... 0 0 1]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matriks Data Testing :\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_recall_fscore_support, classification_report \n",
    "\n",
    "nb = naive_bayes.MultinomialNB()\n",
    "nb.fit(X_train, Y_train)\n",
    "\n",
    "pred_y_train = nb.predict(X_train)\n",
    "pred_y_test = nb.predict(X_validation)\n",
    "\n",
    "accuracy_train = accuracy_score(Y_train, pred_y_train)\n",
    "accuracy_test = accuracy_score(Y_validation, pred_y_test)\n",
    "print(\"Accuracy training :\" + str(accuracy_train))\n",
    "print(\"Accuracy testing :\" + str(accuracy_test))\n",
    "print(\"Confusion Matriks Data Training :\\n\" + str(sklearn.metrics.confusion_matrix(Y_train, pred_y_train)))\n",
    "print(\"Confusion Matriks Data Testing :\\n\" + str(sklearn.metrics.confusion_matrix(Y_validation, pred_y_test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/logistic.py:460: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Akurasi Data Training : 0.9988962472406181\n",
      "Akurasi Data Testing : 0.0008576329331046312\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matriks Data Training :\n",
      "[[1 0 0 ... 0 0 0]\n",
      " [0 1 0 ... 0 0 0]\n",
      " [0 0 1 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 1 0 0]\n",
      " [0 0 0 ... 0 1 0]\n",
      " [0 0 0 ... 0 0 1]]\n",
      "Confusion Matriks Data Testing :\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "logreg = LogisticRegression()\n",
    "logreg.fit(X_train, Y_train)\n",
    "y_train_pred = logreg.predict(X_train)\n",
    "y_val_pred = logreg.predict(X_validation)\n",
    "print(\"Akurasi Data Training : \" + str(logreg.score(X_train, Y_train)))\n",
    "print(\"Akurasi Data Testing : \" + str(logreg.score(X_validation, Y_validation)))\n",
    "print(\"Confusion Matriks Data Training :\\n\" + str(sklearn.metrics.confusion_matrix(Y_train, y_train_pred)))\n",
    "print(\"Confusion Matriks Data Testing :\\n\" + str(sklearn.metrics.confusion_matrix(Y_validation, y_val_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "randomforest = RandomForestClassifier(random_state=seed)\n",
    "randomforest.fit(X_train, Y_train)\n",
    "\n",
    "pred_y_train = randomforest.predict(X_train)\n",
    "pred_y_test = randomforest.predict(X_validation)\n",
    "\n",
    "accuracy_train = accuracy_score(Y_train, pred_y_train)\n",
    "accuracy_test = accuracy_score(Y_validation, pred_y_test)\n",
    "print(\"Accuracy training :\" + str(accuracy_train))\n",
    "print(\"Accuracy testing :\" + str(accuracy_test))\n",
    "print(\"Confusion Matriks Data Training :\\n\" + str(sklearn.metrics.confusion_matrix(Y_train, y_train_pred)))\n",
    "print(\"Confusion Matriks Data Testing :\\n\" + str(sklearn.metrics.confusion_matrix(Y_validation, y_val_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(X_train, Y_train)\n",
    "\n",
    "pred_y_train = dt.predict(X_train)\n",
    "pred_y_test = dt.predict(X_validation)\n",
    "\n",
    "accuracy_train = accuracy_score(Y_train, pred_y_train)\n",
    "accuracy_test = accuracy_score(Y_validation, pred_y_test)\n",
    "print(\"Accuracy training :\" + str(accuracy_train))\n",
    "print(\"Accuracy testing :\" + str(accuracy_test))\n",
    "print(\"Confusion Matriks Data Training :\\n\" + str(sklearn.metrics.confusion_matrix(Y_train, y_train_pred)))\n",
    "print(\"Confusion Matriks Data Testing :\\n\" + str(sklearn.metrics.confusion_matrix(Y_validation, y_val_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### SVM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### Linear Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy training :0.9992641648270787\n",
      "Accuracy testing :0.0008576329331046312\n",
      "Confusion Matriks Data Training :\n",
      "[[1 0 0 ... 0 0 0]\n",
      " [0 1 0 ... 0 0 0]\n",
      " [0 0 1 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 1 0 0]\n",
      " [0 0 0 ... 0 1 0]\n",
      " [0 0 0 ... 0 0 1]]\n",
      "Confusion Matriks Data Testing :\n",
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "svc = SVC(kernel=\"linear\")\n",
    "svc.fit(X_train, Y_train)\n",
    "pred_y_train = svc.predict(X_train)\n",
    "pred_y_test = svc.predict(X_validation)\n",
    "\n",
    "accuracy_train = accuracy_score(Y_train, pred_y_train)\n",
    "accuracy_test = accuracy_score(Y_validation, pred_y_test)\n",
    "print(\"Accuracy training :\" + str(accuracy_train))\n",
    "print(\"Accuracy testing :\" + str(accuracy_test))\n",
    "print(\"Confusion Matriks Data Training :\\n\" + str(sklearn.metrics.confusion_matrix(Y_train, y_train_pred)))\n",
    "print(\"Confusion Matriks Data Testing :\\n\" + str(sklearn.metrics.confusion_matrix(Y_validation, y_val_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### Radial Basis Function (RBF) Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "svc_rbf = SVC()\n",
    "svc_rbf.fit(X_train, Y_train)\n",
    "pred_y_train = svc_rbf.predict(X_train)\n",
    "pred_y_test = svc_rbf.predict(X_validation)\n",
    "\n",
    "accuracy_train = accuracy_score(Y_train, pred_y_train)\n",
    "accuracy_test = accuracy_score(Y_validation, pred_y_test)\n",
    "print(\"Accuracy training :\" + str(accuracy_train))\n",
    "print(\"Accuracy testing :\" + str(accuracy_test))\n",
    "print(\"Confusion Matriks Data Training :\\n\" + str(sklearn.metrics.confusion_matrix(Y_train, y_train_pred)))\n",
    "print(\"Confusion Matriks Data Testing :\\n\" + str(sklearn.metrics.confusion_matrix(Y_validation, y_val_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### Polynomial Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "svc_poly = SVC(kernel='poly')\n",
    "svc_poly.fit(X_train, Y_train)\n",
    "pred_y_train = svc_poly.predict(X_train)\n",
    "pred_y_test = svc_poly.predict(X_validation)\n",
    "\n",
    "accuracy_train = accuracy_score(Y_train, pred_y_train)\n",
    "accuracy_test = accuracy_score(Y_validation, pred_y_test)\n",
    "print(\"Accuracy training :\" + str(accuracy_train))\n",
    "print(\"Accuracy testing :\" + str(accuracy_test))\n",
    "print(\"Confusion Matriks Data Training :\\n\" + str(sklearn.metrics.confusion_matrix(Y_train, y_train_pred)))\n",
    "print(\"Confusion Matriks Data Testing :\\n\" + str(sklearn.metrics.confusion_matrix(Y_validation, y_val_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "##### Sigmoid Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "svc_sig = SVC(kernel='sigmoid')\n",
    "svc_sig.fit(X_train, Y_train)\n",
    "pred_y_train = svc_sig.predict(X_train)\n",
    "pred_y_test = svc_sig.predict(X_validation)\n",
    "\n",
    "accuracy_train = accuracy_score(Y_train, pred_y_train)\n",
    "accuracy_test = accuracy_score(Y_validation, pred_y_test)\n",
    "print(\"Accuracy training :\" + str(accuracy_train))\n",
    "print(\"Accuracy testing :\" + str(accuracy_test))\n",
    "print(\"Confusion Matriks Data Training :\\n\" + str(sklearn.metrics.confusion_matrix(Y_train, y_train_pred)))\n",
    "print(\"Confusion Matriks Data Testing :\\n\" + str(sklearn.metrics.confusion_matrix(Y_validation, y_val_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "#### Testing with real data (data_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "saved_model = pickle.dumps(logreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "# Load the pickled model\n",
    "classifier_from_pickle = pickle.loads(saved_model)\n",
    "# Use the loaded pickled model to make predictions of data_test\n",
    "x_test_pred = classifier_from_pickle.predict(X_test_cv)\n",
    "#accuracy\n",
    "result = classifier_from_pickle.score(X_test_cv, test_y)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Word Cloud Representation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "positif = train_sample[\"filtered.reviews\"][train_sample[\"reviews.sentiment\"]=='positive'] \n",
    "negatif = train_sample[\"filtered.reviews\"][train_sample[\"reviews.sentiment\"]=='negative'] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "from wordcloud import WordCloud \n",
    "import matplotlib.pyplot as plt \n",
    "wordcloud = WordCloud(random_state=seed).generate(\" \".join(positif)) \n",
    "plt.figure(figsize = (15,7)) \n",
    "plt.imshow(wordcloud,interpolation = 'bilinear') \n",
    "plt.axis(\"off\") \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
    "from wordcloud import WordCloud \n",
    "import matplotlib.pyplot as plt \n",
    "wordcloud = WordCloud(random_state=seed).generate(\" \".join(negatif)) \n",
    "plt.figure(figsize = (15,7)) \n",
    "plt.imshow(wordcloud,interpolation = 'bilinear') \n",
    "plt.axis(\"off\") \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
   ],
   "source": [
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (Ubuntu Linux)",
   "language": "python",
   "metadata": {
    "cocalc": {
     "description": "Python 3 programming language",
     "priority": 100,
     "url": "https://www.python.org/"
    }
   },
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}